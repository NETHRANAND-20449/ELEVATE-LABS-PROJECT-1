# scanner/crawler.py
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import re
from typing import List, Dict, Set

class WebCrawler:
    def __init__(self, target_url: str, max_depth: int = 2):
        self.target_url = target_url
        self.max_depth = max_depth
        self.visited_urls: Set[str] = set()
        self.found_forms: List[Dict] = []
        self.found_links: List[str] = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'VulnScanner/1.0 (Security Testing Tool)'
        })
        
    def is_valid_url(self, url: str) -> bool:
        """Check if URL belongs to target domain"""
        try:
            target_domain = urlparse(self.target_url).netloc
            url_domain = urlparse(url).netloc
            return target_domain == url_domain
        except:
            return False
    
    def extract_forms(self, soup: BeautifulSoup, url: str) -> List[Dict]:
        """Extract all forms from the page"""
        forms = []
        for form in soup.find_all('form'):
            form_data = {
                'action': urljoin(url, form.get('action', '')),
                'method': form.get('method', 'get').lower(),
                'inputs': [],
                'url': url
            }
            
            # Extract all input fields
            for input_tag in form.find_all(['input', 'textarea', 'select']):
                input_data = {
                    'name': input_tag.get('name', ''),
                    'type': input_tag.get('type', 'text'),
                    'value': input_tag.get('value', ''),
                    'required': input_tag.get('required', False)
                }
                if input_data['name']:
                    form_data['inputs'].append(input_data)
            
            forms.append(form_data)
        return forms
    
    def extract_links(self, soup: BeautifulSoup, current_url: str) -> List[str]:
        """Extract all links from the page"""
        links = []
        for link in soup.find_all('a', href=True):
            full_url = urljoin(current_url, link['href'])
            if self.is_valid_url(full_url) and full_url not in self.visited_urls:
                links.append(full_url)
        return links
    
    def crawl_page(self, url: str) -> bool:
        """Crawl a single page"""
        try:
            print(f"Crawling: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract forms
            forms = self.extract_forms(soup, url)
            self.found_forms.extend(forms)
            
            # Extract links
            links = self.extract_links(soup, url)
            self.found_links.extend(links)
            
            return True
            
        except Exception as e:
            print(f"Error crawling {url}: {str(e)}")
            return False
    
    def start_crawling(self) -> Dict:
        """Start the crawling process"""
        print(f"Starting crawl of {self.target_url}")
        
        urls_to_visit = [self.target_url]
        depth = 0
        
        while urls_to_visit and depth < self.max_depth:
            current_urls = urls_to_visit.copy()
            urls_to_visit = []
            
            for url in current_urls:
                if url not in self.visited_urls:
                    self.visited_urls.add(url)
                    if self.crawl_page(url):
                        time.sleep(0.5)  # Rate limiting
            
            # Add newly found links for next depth level
            for link in self.found_links:
                if link not in self.visited_urls:
                    urls_to_visit.append(link)
            
            depth += 1
        
        return {
            'visited_urls': list(self.visited_urls),
            'forms': self.found_forms,
            'links': self.found_links
        }